{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 📌 Import necessary libraries\n",
        "import numpy as np\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ],
      "metadata": {
        "id": "0EWCjzJQUdF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your dataset file (txt)\n",
        "file_path = '/content/dataset_book.txt'\n",
        "\n",
        "input_texts = []\n",
        "next_words = []\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Assuming format: input sequence + space + next_word\n",
        "        # For example: \"I love to play football\"\n",
        "        parts = line.split()\n",
        "        input_seq = \" \".join(parts[:-1])  # all except last word\n",
        "        next_word = parts[-1]             # last word is next word to predict\n",
        "\n",
        "        input_texts.append(input_seq)\n",
        "        next_words.append(next_word)\n",
        "\n",
        "print(\"Sample input:\", input_texts[1])\n",
        "print(\"Sample next word:\", next_words[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRkfAqaysrWt",
        "outputId": "b61ea1c8-5925-4411-d978-9fef05af4322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input: It is a truth universally acknowledged, that a single man\n",
            "Sample next word: wife.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Import Tokenizer\n",
        "\n",
        "# Convert input texts and next words into sequences\n",
        "\n",
        "# Create and fit tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + next_words) # Fit tokenizer on all text data\n",
        "total_words = len(tokenizer.word_index) + 1 # Get total number of unique words\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "labels = tokenizer.texts_to_sequences(next_words)\n",
        "\n",
        "# Ensure each label is a single integer\n",
        "labels = [label[0] if len(label) > 0 else 0 for label in labels]\n",
        "\n",
        "# Check for empty input sequences\n",
        "if not input_sequences:\n",
        "    print(\"No valid input sequences found in the file.\")\n",
        "    exit()  # or handle appropriately\n",
        "else:\n",
        "    # Pad input sequences to same length\n",
        "    max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "    X = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "    y = to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "olx1Y6mms7uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0afwgPVtCMe",
        "outputId": "90fb2e1e-a8d7-4aa3-ad59-a4eea07d4fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aclJKHectAA_",
        "outputId": "f23616ba-fb0a-495a-9b7f-644c06f5ce29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0,    0],\n",
              "       [   0,    0,    0, ...,    6, 1052,  119],\n",
              "       [   0,    0,    0, ...,  349,    3,    6],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    1,  181,  165],\n",
              "       [   0,    0,    0, ..., 1735,    4,   92],\n",
              "       [   0,    0,    0, ...,  217,  130,  592]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=10, input_length=max_seq_len))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "DsFEI_7UtJni",
        "outputId": "13bd3097-621e-4e01-dca4-75dadc213dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=30, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3mF24DltLTk",
        "outputId": "e47173ba-e5cc-468d-addb-dc550b02b9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m378/378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 64ms/step - accuracy: 0.0379 - loss: 7.3646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x790eec496010>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word for a given input text\n",
        "def predict_next_word(model, tokenizer, text, max_seq_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# Example test\n",
        "input_text = \"You know \"\n",
        "predicted_word = predict_next_word(model, tokenizer, input_text, max_seq_len)\n",
        "print(f\"Input: '{input_text}' -> Predicted next word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_31qMBqvtRKb",
        "outputId": "f3bff99f-4e24-4984-e02a-02b567e6b507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'You know ' -> Predicted next word: 'the'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer From Hugging Face"
      ],
      "metadata": {
        "id": "lpCXwEIu-xJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model_name = \"allenai/t5-small-next-word-generator-qoogle\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def run_model(input_string, **generator_args):\n",
        "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "    res = model.generate(input_ids, **generator_args)\n",
        "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "    print(output)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "I8m0cgXM-0Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"Which\")"
      ],
      "metadata": {
        "id": "-zDq18Zn-1uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/allenai/t5-small-next-word-generator-qoogle"
      ],
      "metadata": {
        "id": "OoXBHhsV_Bwh"
      }
    }
  ]
}